# Mistral Vibe Configuration for Devstral 2 Small
active_model = "devstral-2-small"
system_prompt_id = "vibe_precision"
auto_compact_threshold = 32000    # Frequent compaction to keep context dense
include_model_info = true
include_project_context = true

[[providers]]
name = "openai-local"
api_base = "http://localhost:8080/v1" # Replace with your endpoint
api_key_env_var = "OPENAI_API_KEY"
api_style = "openai"
backend = "generic"
# Recommended: disable streaming if tool calls are unstable
# stream = false 

# --- MCP SERVERS (Local, Open-Source, No API Keys) ---

[[mcp_servers]]
name = "sequential_thinking"
command = "npx"
args = ["-y", "@modelcontextprotocol/server-sequential-thinking"]

[[mcp_servers]]
name = "context7"
command = "npx"
args = ["-y", "context7-mcp"]

[[mcp_servers]]
name = "openwebsearch"
command = "npx"
args = ["-y", "@modelcontextprotocol/server-fetch"]

# ADDITION 1: Git MCP for factual version control state
[[mcp_servers]]
name = "git"
command = "uvx"
args = ["mcp-server-git"]

# ADDITION 2: Filesystem MCP for precise file tree navigation
[[mcp_servers]]
name = "filesystem"
command = "npx"
args = ["-y", "@modelcontextprotocol/server-filesystem", "."]

[tools]
# Ensure small model doesn't over-rely on bash for simple reads
disabled_tools = [] 
[tools.bash]
permission = "ask" # Safety first for smaller models